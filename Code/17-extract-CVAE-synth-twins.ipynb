{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21eb8294-68cc-4f4a-a57f-b69daf6af260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb  3 08:19:51 EST 2024\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc3a83f-81ca-4ccd-b535-d9bc6e696411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mmfs1/data/aglinska/BC-sim/Code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b818a5-3d94-4678-a8f1-0b2acc69bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 14:44:14.750211: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "/data/aglinska/anaconda3/envs/tf231/lib/python3.8/site-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.18.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from rsa_funcs import fit_rsa,make_RDM,get_triu\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "now = datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507fde91-2e12-41d4-90ab-c9ce69b90d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "      args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "      z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def get_MRI_CVAE_3D(input_shape=(64,64,64,1),\n",
    "                    latent_dim=2,\n",
    "                    beta=1,\n",
    "                    disentangle=False,\n",
    "                    gamma=1,\n",
    "                    bias=True,\n",
    "                    batch_size = 64,\n",
    "                    kernel_size = 3,\n",
    "                    filters = 32,\n",
    "                    intermediate_dim = 128,\n",
    "                    opt=None):\n",
    "\n",
    "    image_size, _, _, channels = input_shape\n",
    "    #epochs = 10\n",
    "    nlayers = 2\n",
    "\n",
    "    # build encoder model\n",
    "    tg_inputs = Input(shape=input_shape, name='tg_inputs')\n",
    "    bg_inputs = Input(shape=input_shape, name='bg_inputs')\n",
    "\n",
    "    z_conv1 = Conv3D(filters=filters*2,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides=2,\n",
    "            use_bias=bias,\n",
    "            padding='same')\n",
    "\n",
    "    z_conv2 = Conv3D(filters=filters*4,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides=2,\n",
    "            use_bias=bias,\n",
    "            padding='same')\n",
    "\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    z_h_layer = Dense(intermediate_dim, activation='relu', use_bias=bias)\n",
    "    z_mean_layer = Dense(latent_dim, name='z_mean', use_bias=bias)\n",
    "    z_log_var_layer = Dense(latent_dim, name='z_log_var', use_bias=bias)\n",
    "    z_layer = Lambda(sampling, output_shape=(latent_dim,), name='z')\n",
    "\n",
    "    def z_encoder_func(inputs):\n",
    "        z_h = inputs\n",
    "        z_h = z_conv1(z_h)\n",
    "        z_h = z_conv2(z_h)\n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(z_h)\n",
    "        z_h = Flatten()(z_h)\n",
    "        z_h = z_h_layer(z_h)\n",
    "        z_mean =  z_mean_layer(z_h)\n",
    "        z_log_var =  z_log_var_layer(z_h)\n",
    "        z = z_layer([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z, shape\n",
    "\n",
    "    tg_z_mean, tg_z_log_var, tg_z, shape_z = z_encoder_func(tg_inputs)\n",
    "\n",
    "\n",
    "    s_conv1 = Conv3D(filters=filters*2,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides=2,\n",
    "            use_bias=bias,\n",
    "            padding='same')\n",
    "\n",
    "    s_conv2 = Conv3D(filters=filters*4,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            strides=2,\n",
    "            use_bias=bias,\n",
    "            padding='same')\n",
    "\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    s_h_layer = Dense(intermediate_dim, activation='relu', use_bias=bias)\n",
    "    s_mean_layer = Dense(latent_dim, name='s_mean', use_bias=bias)\n",
    "    s_log_var_layer = Dense(latent_dim, name='s_log_var', use_bias=bias)\n",
    "    s_layer = Lambda(sampling, output_shape=(latent_dim,), name='s')\n",
    "\n",
    "    def s_encoder_func(inputs):\n",
    "        s_h = inputs\n",
    "        s_h = s_conv1(s_h)\n",
    "        s_h = s_conv2(s_h)\n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(s_h)\n",
    "        s_h = Flatten()(s_h)\n",
    "        s_h = s_h_layer(s_h)\n",
    "        s_mean =  s_mean_layer(s_h)\n",
    "        s_log_var =  s_log_var_layer(s_h)\n",
    "        s = s_layer([s_mean, s_log_var])\n",
    "        return s_mean, s_log_var, s, shape\n",
    "\n",
    "    tg_s_mean, tg_s_log_var, tg_s, shape_s = s_encoder_func(tg_inputs)\n",
    "    #bg_s_mean, bg_s_log_var, bg_s, _ = s_encoder_func(bg_inputs) # this is what they had \n",
    "    bg_z_mean, bg_z_log_var, bg_z, _ = z_encoder_func(bg_inputs) # Aidas and Stefano team hax\n",
    "\n",
    "\n",
    "      # instantiate encoder models\n",
    "    z_encoder = tf.keras.models.Model(tg_inputs, [tg_z_mean, tg_z_log_var, tg_z], name='z_encoder')\n",
    "    s_encoder = tf.keras.models.Model(tg_inputs, [tg_s_mean, tg_s_log_var, tg_s], name='s_encoder')\n",
    "\n",
    "\n",
    "      # build decoder model\n",
    "    latent_inputs = Input(shape=(2*latent_dim,), name='z_sampling')\n",
    "\n",
    "    x = Dense(intermediate_dim, activation='relu', use_bias=bias)(latent_inputs)\n",
    "    x = Dense(shape_z[1] * shape_z[2] * shape_z[3] * shape_z[4], activation='relu', use_bias=bias)(x)\n",
    "    x = Reshape((shape_z[1], shape_z[2], shape_z[3],shape_z[4]))(x)\n",
    "\n",
    "    for i in range(nlayers):\n",
    "        x = Conv3DTranspose(filters=filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          strides=2,\n",
    "                          use_bias=bias,\n",
    "                          padding='same')(x)\n",
    "        filters //= 2\n",
    "\n",
    "    outputs = Conv3DTranspose(filters=1,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='sigmoid',\n",
    "                            padding='same',\n",
    "                            use_bias=bias,\n",
    "                            name='decoder_output')(x)\n",
    "\n",
    "    # instantiate decoder model\n",
    "    cvae_decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "      # decoder.summary()\n",
    "\n",
    "    def zeros_like(x):\n",
    "        return tf.zeros_like(x)\n",
    "\n",
    "    tg_outputs = cvae_decoder(tf.keras.layers.concatenate([tg_z, tg_s], -1))\n",
    "    zeros = tf.keras.layers.Lambda(zeros_like)(tg_z)\n",
    "\n",
    "    bg_outputs = cvae_decoder(tf.keras.layers.concatenate([bg_z, zeros], -1)) # Aidas look into this, is this correct\n",
    "\n",
    " #   fg_outputs = cvae_decoder(tf.keras.layers.concatenate([tg_z, zeros], -1))\n",
    "\n",
    "    # instantiate VAE model\n",
    "    cvae = tf.keras.models.Model(inputs=[tg_inputs, bg_inputs], \n",
    "                              outputs=[tg_outputs, bg_outputs], \n",
    "                              name='contrastive_vae')\n",
    "\n",
    "#     cvae_fg = tf.keras.models.Model(inputs=tg_inputs, \n",
    "#                                   outputs=fg_outputs, \n",
    "#                                   name='contrastive_vae_fg')\n",
    "\n",
    "    if disentangle:\n",
    "        discriminator = Dense(1, activation='sigmoid')\n",
    "\n",
    "        z1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_z)\n",
    "        z2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_z)\n",
    "        s1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_s)\n",
    "        s2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_s)\n",
    "\n",
    "        q_bar = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        # q_bar_score = (discriminator(q_bar)) # +.1 * .85 so that it's 0<x<1 # assuming joint s z distribution\n",
    "        # q_score = (discriminator(q)) # assuming that they're indepoendent \n",
    "\n",
    "        q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1 # assuming joint s z distribution\n",
    "        q_score = (discriminator(q)+.1) *.85 # assuming that they're indepoendent \n",
    "        \n",
    "        tc_loss = K.log(q_score / (1 - q_score)) \n",
    "        discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "    else:\n",
    "        tc_loss = 0\n",
    "        discriminator_loss = 0\n",
    "\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(tg_inputs), K.flatten(tg_outputs)) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(bg_inputs), K.flatten(bg_outputs)) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2] * input_shape[3]\n",
    "\n",
    "\n",
    "    kl_loss = 1 + tg_z_log_var - tf.keras.backend.square(tg_z_mean) - tf.keras.backend.exp(tg_z_log_var)\n",
    "    kl_loss += 1 + tg_s_log_var - tf.keras.backend.square(tg_s_mean) - tf.keras.backend.exp(tg_s_log_var)\n",
    "    kl_loss += 1 + bg_z_log_var - tf.keras.backend.square(bg_z_mean) - tf.keras.backend.exp(bg_z_log_var)\n",
    "    kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    \n",
    "    #print(f'reconstruction loss {reconstruction_loss}')\n",
    "    #print(f'kl_loss loss {kl_loss}')\n",
    "    #print(f'tc_loss loss {tc_loss}')\n",
    "    #print(f'discriminator_loss loss {discriminator_loss}')\n",
    "    \n",
    "    cvae_loss = tf.keras.backend.mean(reconstruction_loss + beta*kl_loss + gamma*tc_loss + gamma*discriminator_loss) # if increasing TC loss, might be a good idea to also increase DC loss (discriminator_loss*gamma)\n",
    "    cvae.add_loss(cvae_loss)\n",
    "    \n",
    "    if type(opt)==type(None):\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')\n",
    "    \n",
    "#     opt = tf.keras.optimizers.SGD(\n",
    "#     learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD')\n",
    "\n",
    "    #opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.9, epsilon=1e-07, centered=False, name='RMSprop')\n",
    "    \n",
    "    #cvae.compile(optimizer='rmsprop',run_eagerly=True)\n",
    "    cvae.compile(optimizer=opt,run_eagerly=True)\n",
    "    \n",
    "\n",
    "    #return cvae, cvae_fg, z_encoder, s_encoder, cvae_decoder\n",
    "    return cvae, z_encoder, s_encoder, cvae_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa2e450-ea60-41ec-8763-5e009bc87632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_chunks(l, n):\n",
    "    # Yield successive n-sized\n",
    "    # chunks from l.\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e259ae-2412-40ea-8719-880f18314265",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30609d91-ecfe-45c3-94c2-959d307076cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 14:44:18.109376: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2024-02-02 14:44:18.143188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.77GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-02-02 14:44:18.143211: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-02 14:44:18.145080: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-02 14:44:18.146986: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2024-02-02 14:44:18.147543: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2024-02-02 14:44:18.148841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-02-02 14:44:18.149754: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-02-02 14:44:18.152641: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-02 14:44:18.153118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2024-02-02 14:44:18.153669: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 14:44:18.158705: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2400000000 Hz\n",
      "2024-02-02 14:44:18.158792: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555556319f90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-02 14:44:18.158798: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-02-02 14:44:18.234006: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555557c11220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-02 14:44:18.234023: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "2024-02-02 14:44:18.234386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.77GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2024-02-02 14:44:18.234410: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-02 14:44:18.234430: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-02 14:44:18.234437: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2024-02-02 14:44:18.234443: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2024-02-02 14:44:18.234450: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-02-02 14:44:18.234456: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-02-02 14:44:18.234463: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-02 14:44:18.234885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2024-02-02 14:44:18.234905: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-02-02 14:44:18.632180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-02-02 14:44:18.632210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2024-02-02 14:44:18.632215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2024-02-02 14:44:18.632855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14755 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:18:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "cvae, z_encoder, s_encoder, cvae_decoder = get_MRI_CVAE_3D(input_shape=(64,64,64,1),\n",
    "                latent_dim=2,\n",
    "                beta = 1,\n",
    "                disentangle=True,\n",
    "                gamma= 100,\n",
    "                bias=True,\n",
    "                batch_size = batch_size,\n",
    "                kernel_size = 3,\n",
    "                filters = 32,\n",
    "                intermediate_dim = 128,\n",
    "                opt=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cafa536d-3c59-4121-aa0a-b2eb519e0744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "#data_fn = '../Data/synth-data-02.npy' # baeline\n",
    "#data_fn = '../synth-data-01-repl-change-S/data.npy' #repl-same\n",
    "#data_fn = '../synth-data-05/data.npy'\n",
    "#data_fn = '../synth-data-03/data.npy'\n",
    "data_fn = '../Data/synth-data-02-N1000.npy'\n",
    "data = np.load(data_fn)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd39bab2-5f73-462a-94f6-a51bd5f4ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latents_z = np.zeros((5,20,500,2))\n",
    "# latents_s = np.zeros((5,20,500,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e18d08-704d-47ac-a8c6-5960eb017ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyses = ['good-WGEYQtcZZJ','good-mbKzlWMysG','good-CfFxxcHpOS','good-QQXNZRBSTz','good-CIMgmCzfEA']\n",
    "# for i in range(5):\n",
    "#     analysis = analyses[i]\n",
    "    \n",
    "#     indir = f'../scratch/{analysis}/stage_2/'\n",
    "#     weights_fn = [os.path.join(indir,f.split('.')[0]) for f in os.listdir(indir) if f.endswith('.index')]\n",
    "\n",
    "#     for j in tqdm(range(20)):\n",
    "#         these_weights = weights_fn[j]\n",
    "#         cvae.load_weights(these_weights)\n",
    "        \n",
    "#         z_all = z_encoder.predict(data[0:500,:,:,:])[0]\n",
    "#         s_all = s_encoder.predict(data[0:500,:,:,:])[0]\n",
    "        \n",
    "#         latents_z[i,j,:,:] = z_all\n",
    "#         latents_s[i,j,:,:] = s_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25dd2846-3495-46e2-81ce-f02e015181e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyses = ['good-WGEYQtcZZJ','good-mbKzlWMysG','good-CfFxxcHpOS','good-QQXNZRBSTz','good-CIMgmCzfEA']\n",
    "#analyses = ['CVAE-5type-dTkDjgdfxL','CVAE-5type-iAiIvvxnyA','CVAE-5type-HyABYkiDmv','CVAE-5type-uLtXBzUGUc','CVAE-5type-IFVZazjuWp']\n",
    "#analyses = ['CVAE-3type-RpjbVewTkF','CVAE-3type-COlRaiafwG','CVAE-3type-rxbrpNgKNC','CVAE-3type-cjDXIoxZHB','CVAE-3type-YAjYFtDAep']\n",
    "#analyses = ['2type-ImTsKjutaD','2type-yvdvqaCvSX','2type-GCyPfzEyNN','2type-odHQZhUYxZ','2type-QzJmjkwddT']\n",
    "analyses = '2type-N1000-SsAkmZgMEg','2type-N1000-jgaXcHvlrU','2type-N1000-yVufeEzKhJ','2type-N1000-lNlPUEKniS','2type-N1000-RyexWBHBsC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52722b74-24d0-4ac3-a209-422f81042eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "suffix = '-02-N1000'\n",
    "save_dir = f'recons_CVAE{suffix}'\n",
    "if not os.path.exists(os.path.join('../Data/',save_dir)):\n",
    "    os.mkdir(os.path.join('../Data/',save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "730ee75b-a905-4809-a9af-091a71eee951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]2024-02-02 14:44:24.535451: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2024-02-02 14:44:24.743342: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-02-02 14:44:25.617583: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256\n",
      "2024-02-02 14:44:25.655669: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "100%|███████████████████████████████████████████| 20/20 [11:17<00:00, 33.88s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [11:13<00:00, 33.66s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [11:14<00:00, 33.73s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [11:13<00:00, 33.69s/it]\n",
      "100%|███████████████████████████████████████████| 20/20 [11:08<00:00, 33.42s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "#i = 4\n",
    "    analysis = analyses[i]\n",
    "    indir = f'../scratch/{analysis}/stage_2/'\n",
    "    weights_fn = [os.path.join(indir,f.split('.')[0]) for f in os.listdir(indir) if f.endswith('.index')]\n",
    "    for j in tqdm(range(len(weights_fn))):\n",
    "        these_weights = weights_fn[j]\n",
    "        cvae.load_weights(these_weights)\n",
    "        \n",
    "        z_all = z_encoder.predict(data[0:1000,:,:,:])[0]\n",
    "        s_all = s_encoder.predict(data[0:1000,:,:,:])[0]\n",
    "        chunks = list(divide_chunks(np.arange(1000), 50))\n",
    "        reconstructions = np.concatenate([cvae_decoder.predict(np.hstack((z_all[chunk,:],s_all[chunk,:])))[:,:,:,:,0] for chunk in chunks],axis=0)\n",
    "        twins = np.concatenate([cvae_decoder.predict(np.hstack((z_all[chunk,:],np.zeros((50,2)) )))[:,:,:,:,0] for chunk in chunks],axis=0)\n",
    "        \n",
    "        np.save(file=f'../Data/{save_dir}/ens-{i}-model-{j}-recons.npy',arr=reconstructions.astype(np.float16))\n",
    "        np.save(file=f'../Data/{save_dir}/ens-{i}-model-{j}-twins.npy',arr=twins.astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2dec01-d385-4e24-8b15-9c114046faf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f233930c-58e6-4c3e-ae5d-d437e9e733bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220b64f7-ca71-4bbf-9b3e-031be069977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:16<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "recons = [[np.load(f'../Data/{save_dir}/ens-{i}-model-{j}-recons.npy') for j in range(20)] for i in tqdm(range(5))]\n",
    "recons = np.array(recons)\n",
    "np.save(file=f'../Data/CVAE_recons{suffix}.npy',arr=recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb185d61-d584-4160-bdf7-ea565ed26d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a8827f-1cec-4641-a6cd-5be0b4031c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:16<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "twin = [[np.load(f'../Data/{save_dir}/ens-{i}-model-{j}-twins.npy') for j in range(20)] for i in tqdm(range(5))]\n",
    "twin = np.array(twin)\n",
    "np.save(file=f'../Data/CVAE_twins{suffix}.npy',arr=twin)\n",
    "del twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b04ddb53-d756-45b4-a912-5ba3031e3f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [06:51<00:00, 82.38s/it]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "diff = [[np.load(f'../Data/{save_dir}/ens-{i}-model-{j}-recons.npy')-np.load(f'../Data/{save_dir}/ens-{i}-model-{j}-twins.npy') for j in range(20)] for i in tqdm(range(5))]\n",
    "diff = np.array(diff)\n",
    "np.save(file=f'../Data/CVAE_diffs{suffix}.npy',arr=diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acd7c8d3-2db5-4c55-846c-90cd5ddd24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file=f'../Data/CVAE_diffs_mean{suffix}.npy',arr=diff.mean(axis=0).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d519db2-1188-45f7-befb-49f3dd267252",
   "metadata": {},
   "outputs": [],
   "source": [
    "del diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c0d9a-7088-4b07-9754-8186bf6574fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(file='../Data/repl_latents/repl-changeS-z.npy',arr=latents_z)\n",
    "# np.save(file='../Data/repl_latents/repl-changeS-s.npy',arr=latents_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d77ee-91b8-44f0-8df9-5aa44f143915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba637a0-717c-45ee-a7ac-7cf2291474b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9f06a-020f-4185-88c0-e78d6c6bd802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04d76a-19cd-4180-8cde-6f7cf49f0cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9014e-6e21-44e8-b2b6-28b0305dd79d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
